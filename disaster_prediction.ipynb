{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KUQeAZyFBFNE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import LSTM , Dense , GlobalMaxPooling1D\n",
        "\n",
        "from tensorflow.keras.layers import Input , Embedding\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input, Embedding, GlobalMaxPooling1D, Dropout, Bidirectional, Conv1D, MaxPooling1D, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input, Embedding, GlobalMaxPooling1D, Dropout, Bidirectional, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input, Embedding, GlobalMaxPooling1D, Dropout\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input, Embedding, GlobalMaxPooling1D, Dropout, Bidirectional, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input, Embedding, GlobalMaxPooling1D, Dropout\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "df = data\n",
        "df = df.drop(['id','keyword','place'] , axis =  1)\n",
        "df.columns = ['data','label']\n",
        "y = df.iloc[:,1].values\n",
        "x = df.iloc[:,0].values\n",
        "\n",
        "x_train , x_test , y_train , y_test = train_test_split(x,y,test_size =0.2 , random_state = 42)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(filepath='/best_weights.keras',\n",
        "                                          monitor='val_accuracy',\n",
        "                                          verbose=1,\n",
        "                                          save_best_only=True,\n",
        "                                          mode='max')\n",
        "\n",
        "MAX_VOCAB_SIZE = 20000\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "\n",
        "# Fit tokenizer on training data\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "# Convert text to sequences of word indices\n",
        "sequences_train = tokenizer.texts_to_sequences(x_train)\n",
        "sequences_test = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "data_train = pad_sequences(sequences_train)\n",
        "print(data_train.shape)\n",
        "T = data_train.shape[1]\n",
        "\n",
        "data_test = pad_sequences(sequences_test , maxlen = T)\n",
        "print(data_test.shape)\n",
        "\n",
        "\n",
        "word2idx = tokenizer.word_index\n",
        "v = len(word2idx)\n",
        "\n",
        "D = 20\n",
        "M = 15\n",
        "\n",
        "# Define a callback to save the weights with the highest validation accuracy\n",
        "checkpoint_callback = ModelCheckpoint(filepath='/best_weights.keras',\n",
        "                                      monitor='val_accuracy',\n",
        "                                      verbose=1,\n",
        "                                      save_best_only=True,\n",
        "                                      mode='max')\n",
        "\n",
        "\n",
        "D = 20\n",
        "M = 15\n",
        "\n",
        "i = Input(shape = (T,))\n",
        "x = Embedding(v +1 ,D)(i)\n",
        "x = LSTM(128, return_sequences=True)(x)\n",
        "x = GlobalMaxPooling1D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1 , activation  = 'sigmoid')(x)\n",
        "model = Model(i , x)\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Define early stopping callback\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    data_train,\n",
        "    y_train,\n",
        "    epochs=20,  # Increase epochs to allow more training\n",
        "    validation_data=(data_test, y_test),\n",
        "    callbacks=[checkpoint_callback]\n",
        ")\n",
        "\n",
        "model.load_weights(\"/best_weights.keras\")\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "loss, accuracy = model.evaluate(data_test, y_test)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(data_test)\n",
        "\n",
        "# Round the predictions to get binary values\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fJgKCJRYHhBv",
        "outputId": "f37d1240-9168-435a-f767-22751f38f706"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6090, 33)\n",
            "(1523, 33)\n",
            "Epoch 1/20\n",
            "190/191 [============================>.] - ETA: 0s - loss: 0.6195 - accuracy: 0.6470\n",
            "Epoch 1: val_accuracy improved from -inf to 0.79842, saving model to /best_weights.keras\n",
            "191/191 [==============================] - 15s 64ms/step - loss: 0.6192 - accuracy: 0.6473 - val_loss: 0.4690 - val_accuracy: 0.7984\n",
            "Epoch 2/20\n",
            "191/191 [==============================] - ETA: 0s - loss: 0.3582 - accuracy: 0.8548\n",
            "Epoch 2: val_accuracy improved from 0.79842 to 0.80893, saving model to /best_weights.keras\n",
            "191/191 [==============================] - 12s 61ms/step - loss: 0.3582 - accuracy: 0.8548 - val_loss: 0.4584 - val_accuracy: 0.8089\n",
            "Epoch 3/20\n",
            "191/191 [==============================] - ETA: 0s - loss: 0.2022 - accuracy: 0.9278\n",
            "Epoch 3: val_accuracy did not improve from 0.80893\n",
            "191/191 [==============================] - 12s 61ms/step - loss: 0.2022 - accuracy: 0.9278 - val_loss: 0.5295 - val_accuracy: 0.7905\n",
            "Epoch 4/20\n",
            "191/191 [==============================] - ETA: 0s - loss: 0.1053 - accuracy: 0.9640\n",
            "Epoch 4: val_accuracy did not improve from 0.80893\n",
            "191/191 [==============================] - 13s 69ms/step - loss: 0.1053 - accuracy: 0.9640 - val_loss: 0.6719 - val_accuracy: 0.7859\n",
            "Epoch 5/20\n",
            "190/191 [============================>.] - ETA: 0s - loss: 0.0521 - accuracy: 0.9836\n",
            "Epoch 5: val_accuracy did not improve from 0.80893\n",
            "191/191 [==============================] - 11s 57ms/step - loss: 0.0520 - accuracy: 0.9836 - val_loss: 0.8500 - val_accuracy: 0.7728\n",
            "Epoch 6/20\n",
            "191/191 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9918\n",
            "Epoch 6: val_accuracy did not improve from 0.80893\n",
            "191/191 [==============================] - 12s 65ms/step - loss: 0.0269 - accuracy: 0.9918 - val_loss: 0.8030 - val_accuracy: 0.7603\n",
            "Epoch 7/20\n",
            "190/191 [============================>.] - ETA: 0s - loss: 0.0197 - accuracy: 0.9944\n",
            "Epoch 7: val_accuracy did not improve from 0.80893\n",
            "191/191 [==============================] - 12s 65ms/step - loss: 0.0197 - accuracy: 0.9944 - val_loss: 0.9835 - val_accuracy: 0.7649\n",
            "Epoch 8/20\n",
            "191/191 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9949\n",
            "Epoch 8: val_accuracy did not improve from 0.80893\n",
            "191/191 [==============================] - 13s 66ms/step - loss: 0.0168 - accuracy: 0.9949 - val_loss: 1.0138 - val_accuracy: 0.7426\n",
            "Epoch 9/20\n",
            "190/191 [============================>.] - ETA: 0s - loss: 0.0137 - accuracy: 0.9954\n",
            "Epoch 9: val_accuracy did not improve from 0.80893\n",
            "191/191 [==============================] - 12s 65ms/step - loss: 0.0137 - accuracy: 0.9954 - val_loss: 1.1897 - val_accuracy: 0.7617\n",
            "Epoch 10/20\n",
            "191/191 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9959\n",
            "Epoch 10: val_accuracy did not improve from 0.80893\n",
            "191/191 [==============================] - 12s 62ms/step - loss: 0.0112 - accuracy: 0.9959 - val_loss: 1.2981 - val_accuracy: 0.7420\n",
            "Epoch 11/20\n",
            "190/191 [============================>.] - ETA: 0s - loss: 0.0164 - accuracy: 0.9952\n",
            "Epoch 11: val_accuracy did not improve from 0.80893\n",
            "191/191 [==============================] - 12s 62ms/step - loss: 0.0164 - accuracy: 0.9952 - val_loss: 1.1005 - val_accuracy: 0.7557\n",
            "Epoch 12/20\n",
            "191/191 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9951\n",
            "Epoch 12: val_accuracy did not improve from 0.80893\n",
            "191/191 [==============================] - 11s 55ms/step - loss: 0.0134 - accuracy: 0.9951 - val_loss: 1.2517 - val_accuracy: 0.7518\n",
            "Epoch 13/20\n",
            "190/191 [============================>.] - ETA: 0s - loss: 0.0146 - accuracy: 0.9954\n",
            "Epoch 13: val_accuracy did not improve from 0.80893\n",
            "191/191 [==============================] - 12s 63ms/step - loss: 0.0146 - accuracy: 0.9954 - val_loss: 1.0170 - val_accuracy: 0.7643\n",
            "Epoch 14/20\n",
            "191/191 [==============================] - ETA: 0s - loss: 0.0106 - accuracy: 0.9966\n",
            "Epoch 14: val_accuracy did not improve from 0.80893\n",
            "191/191 [==============================] - 12s 65ms/step - loss: 0.0106 - accuracy: 0.9966 - val_loss: 1.2498 - val_accuracy: 0.7551\n",
            "Epoch 15/20\n",
            "191/191 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9961\n",
            "Epoch 15: val_accuracy did not improve from 0.80893\n",
            "191/191 [==============================] - 12s 62ms/step - loss: 0.0100 - accuracy: 0.9961 - val_loss: 1.0933 - val_accuracy: 0.7446\n",
            "Epoch 16/20\n",
            "190/191 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9959\n",
            "Epoch 16: val_accuracy did not improve from 0.80893\n",
            "191/191 [==============================] - 14s 73ms/step - loss: 0.0085 - accuracy: 0.9959 - val_loss: 1.3252 - val_accuracy: 0.7571\n",
            "Epoch 17/20\n",
            "190/191 [============================>.] - ETA: 0s - loss: 0.0083 - accuracy: 0.9962\n",
            "Epoch 17: val_accuracy did not improve from 0.80893\n",
            "191/191 [==============================] - 12s 64ms/step - loss: 0.0083 - accuracy: 0.9962 - val_loss: 1.2319 - val_accuracy: 0.7426\n",
            "Epoch 18/20\n",
            "190/191 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9967\n",
            "Epoch 18: val_accuracy did not improve from 0.80893\n",
            "191/191 [==============================] - 13s 66ms/step - loss: 0.0071 - accuracy: 0.9967 - val_loss: 1.3334 - val_accuracy: 0.7584\n",
            "Epoch 19/20\n",
            "191/191 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9962\n",
            "Epoch 19: val_accuracy did not improve from 0.80893\n",
            "191/191 [==============================] - 11s 59ms/step - loss: 0.0072 - accuracy: 0.9962 - val_loss: 1.4734 - val_accuracy: 0.7584\n",
            "Epoch 20/20\n",
            "191/191 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.9957\n",
            "Epoch 20: val_accuracy did not improve from 0.80893\n",
            "191/191 [==============================] - 11s 57ms/step - loss: 0.0077 - accuracy: 0.9957 - val_loss: 1.3690 - val_accuracy: 0.7498\n",
            "48/48 [==============================] - 1s 16ms/step - loss: 0.4584 - accuracy: 0.8089\n",
            "48/48 [==============================] - 1s 17ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'f1_score' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0d665d7a47e7>\u001b[0m in \u001b[0;36m<cell line: 89>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# Compute F1 score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# Compute mean squared error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'f1_score' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, mean_squared_error\n",
        "\n",
        "# Compute F1 score\n",
        "f1 = f1_score(y_test, y_pred_binary)\n",
        "\n",
        "# Compute mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"Test loss:\", loss)\n",
        "print(\"Test accuracy:\", accuracy)\n",
        "print(\"F1 score:\", f1)\n",
        "print(\"Mean squared error:\", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kksu4T1EJtG5",
        "outputId": "d1bf731e-5400-4a52-e517-411e38c03188"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.4584296941757202\n",
            "Test accuracy: 0.8089297413825989\n",
            "F1 score: 0.738544474393531\n",
            "Mean squared error: 0.14499492652659252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ot365LdpLW7o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}